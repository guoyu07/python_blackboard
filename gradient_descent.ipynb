{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降\n",
    "我们使用梯度下降法求一个线性方程的参数，对于方程 $y=f(x)=\\theta_0 + \\theta_1x $ ，***y***、***x*** 是已知的（我们收集到的数据），***f*** 是已知的（实际是我们假设的函数模型），$\\theta_0、\\theta_1$ 是未知的，是我们要求解的目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降数学推导\n",
    "\n",
    "### 设定\n",
    "\n",
    "> x表示输入变量，称之为特征（features），y表示训练的目标数据（target）\n",
    "\n",
    "> 一对 $\\{x^{(i)},y^{(i)}\\}$ 称之为训练样本(training example)  \n",
    "\n",
    "> 一个m个训练样本的列表 $\\{x^{(i)},y^{(i)}; i = 1, \\cdots, m\\}$ 称之为训练集合(training example)，***m*** 为样本数\n",
    "\n",
    "> $x^{(i)}、y^{(i)}$中 (***i***) 为上标，不是求幂运算的指数，这种写法沿用广为认知的吴恩达机器学习课程中写法，指训练集合中第 ***i*** 组样本。**要注意 *x* 是一个向量，一组 *x* 可能包含了多个特征，一组 *x* 内的特征用下标表示，这里用 *n* 表示**   \n",
    "\n",
    "> 假设函数：由于历史原因把预测器称之为假设（hypothesis），假设函数就是预测函数，用 ***h(x)*** 表示\n",
    "\n",
    "### 假设函数 \n",
    "\n",
    "$\\begin{align}\n",
    "\\large h_\\theta(x) & \\large = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n = \\theta_0 + \\sum\\limits_{i=1}^{n} \\theta_i x_i \\\\\n",
    "\\large 为了简化表示，令常数 x_0 & \\large = 1 ,则 \\\\ \n",
    "\\large h_\\theta(x) & \\large = \\theta_0 \\color{red}{x_0} + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n = \\sum\\limits_{i=\\color{red}{0} }^{n} \\theta_i x_i\\quad \\small \\color{gray}{n为输入变量x的数量（不计x_0）,即特征数量} \\\\\n",
    "\\large & \\large = \\left[\\theta_0, \\theta_1, \\theta_2, \\dots, \\theta_n, \\right]^T \\cdot \\left[ x_0,x_1,x_2,\\cdots,x_n \\right] \\small \\color{gray}{<=用向量方式表示}\\\\\n",
    "\\\\\n",
    "\\large & \\large = {\\boldsymbol{\\theta}^T} \\bf{x} \\quad \\small \\color{gray}{<=此处\\theta,x是向量} \\\\\n",
    "\\end{align}$ \n",
    "\n",
    "### 代价函数\n",
    "代价函数(cost function)用于评估预测值与真实值之间的差距，评估指标有很多，比如标准差、方差、均方误差等。通过**不断调整预测函数的** $\\theta$ 以降低误差，当预测值与真实值之间的差距降低到可接受的范围内时，误差收敛，将 $\\theta$ 代入预测函数，认为这个函数可以描述样本数据的规律。\n",
    "\n",
    "这里采用**方差**评估预测效果，<font color=\"red\"> **对于每个（或者说某个）**</font> $\\color{red}{\\theta}$ ，我们定义一个代价函数 $J(\\theta)$ ，***m*** 为样本数：  \n",
    "$\\large \\quad J(\\theta)=\\frac{1}{2} \\sum\\limits_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 \\quad \\color{gray}{\\small <= \\frac{1}{2} 是为了求偏导数后简化等式，不影响代价函数}$  \n",
    "\n",
    "这个代价函数意义为：将所有样本数据代入假设函数做预测，对于每个（或者说某个） $\\theta$，把每个样本的预测值与真实值的方差加和，作为评估预测效果的指标。\n",
    "\n",
    "### 梯度的数学推导\n",
    "梯度为代价函数$J(\\theta)$求对$\\theta$的偏导数:  \n",
    "\n",
    "$\\begin{align}\n",
    "&\\large \\frac{\\partial J(\\theta)}{\\partial\\theta} \\\\\n",
    "\\\\\n",
    "\\large = & \\large \\frac{\\partial \\left(\\frac{1}{2} \\sum\\limits_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 \\right)}{\\partial \\theta} \\\\\n",
    "\\large = & \\large \\frac{\\partial \\Bigg( \\frac{1}{2} \\big( (h_\\theta(x^{(1)}) - y^{(1)})^2 + (h_\\theta(x^{(2)}) - y^{(2)})^2 + \\dots + (h_\\theta(x^{(m)}) - y^{(m)})^2 \\big) \\Bigg)}{\\partial\\theta} \\small \\color{gray}{<=代入h_\\theta(x)展开}\\\\\n",
    "\\\\\n",
    "\\large = & \\large \\frac{\\partial \\frac{1}{2} (h_\\theta(x^{(1)}) - y^{(1)})^2}{\\partial \\theta} + \\frac{\\partial \\frac{1}{2}  (h_\\theta(x^{(2)}) - y^{(2)})^2}{\\partial \\theta} + \\dots + \\frac{\\partial \\frac{1}{2} (h_\\theta(x^{(m)}) - y^{(m)})^2}{\\partial \\theta} \\small\\color{gray}{<=函数的和差求导法则:} \\color{blue}{(u \\pm v)^\\prime = u^\\prime \\pm v^\\prime} \\\\\n",
    "\\end{align}$\n",
    "\n",
    "**对任意一项求偏导数，其他项的类似，设 *j* $\\in\\{0,\\dots,n\\}$，则第 *j* 项偏导数如下：**  \n",
    "  \n",
    "$\\begin{align} \n",
    "\\large \\frac{\\partial J(\\theta)}{\\partial\\theta_j} & \\large = \\frac{\\partial \\frac{1}{2} (h_\\theta(x^{(j)}) - y^{(j)})^2}{\\partial \\theta_j} \\\\\n",
    "\\large & \\large = \\frac{1}{2} \\cdot 2 \\cdot (h_\\theta(x^{(j)}) - y^{(j)})^{2-1} \\cdot \\frac{ \\partial \\big(h_\\theta (x^{(j)}) - y^{(j)} \\big)}{\\partial \\theta_j} \\\\\n",
    "\\\\\n",
    "\\large & \\large = (h_\\theta(x^{(j)}) - y^{(j)}) \\cdot \\frac{ \\partial(\\theta_0  x^{(j)}_0 + \\theta_1 x^{(j)}_1 + \\dots + \\color{red}{ \\theta_j x^{(j)}_j } + \\dots + \\theta_n x^{(j)}_n - y^j)} {\\partial \\theta_j} \\\\\n",
    "\\large & \\large = (h_\\theta(x^{(j)}) - y^{(j)}) \\cdot x^{(j)} \\qquad \\color{gray}{\\small <=除\\theta_j x^{(j)}_j外，其他项求偏导时都看作常数，常数求导后为0，\\big(\\theta_j x^{(j)}\\big)^\\prime = x^{(j)} }\n",
    "\\end{align}$\n",
    "\n",
    "$\\large \\qquad\\qquad\\qquad \\Downarrow$\n",
    "\n",
    "$\\begin{align}\n",
    "\\large \\frac{\\partial J(\\theta)}{\\partial\\theta} = \\sum\\limits_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x^{(j)}\n",
    "\\end{align}$\n",
    "\n",
    "### 梯度更新算法\n",
    "> 有m个样本\n",
    "\n",
    "> $j\\in\\{1,...,m\\}$,给 $\\theta_j$ 一个随机初始值\n",
    "\n",
    "> 对于所有 $\\theta$，每次一起更新，且更新的梯度都一样\n",
    "\n",
    "> 定义梯度符号为 **∇**，学习速率符号为 **α**，赋值符号为 **:=** \n",
    "\n",
    "更新规则：$\\large \\theta_j := \\theta - \\alpha \\nabla_j $ \n",
    "\n",
    "更新算法如下：\n",
    "            \n",
    "    loop until 误差收敛 {\n",
    "            \n",
    "$\\qquad\\qquad \\theta_j := \\theta - \\alpha \\nabla_j $\n",
    "                  \n",
    "    }\n",
    "    \n",
    "这种更新算法被称为LMS算法，是由Widrow 和 Hoff在1960年的《Adaptive switching circuits》论文中提出的，这种算法广泛应用于自适应滤波器、自适应模式识别等领域。\n",
    "\n",
    "LMS算法是一个搜索算法，假设 $\\theta$ 从某个给定的初始值开始迭代，逐渐使 $J(\\theta)$ 朝着最小的方向变化，直到达到一个值使 $J(\\theta)$ 收敛。\n",
    "> - B.Widrow and M.E.Hoff, “[Adaptive switching circuits](https://www.researchgate.net/publication/230801089_Adaptive_Switching_Circuits),” Proc. Of WESCON Conv. Rec., part 4, pp.96-140, 1960  \n",
    "> - http://www.ee.cityu.edu.hk/~hcso/it6303_4.pdf\n",
    "> - http://blog.csdn.net/xiahouzuoxin/article/details/9749689\n",
    "    \n",
    "#### 批量梯度更新(batch gradient descent)\n",
    "批量梯度更新时，使用全量样本数据计算梯度：$\\large \\nabla_j = \\sum\\limits_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}_j$  \n",
    "\n",
    "更新算法如下：\n",
    "            \n",
    "    loop until 误差收敛 {\n",
    "            \n",
    "$\\qquad\\quad \\theta_j := \\theta_j - \\sum\\limits_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}_j $\n",
    "                  \n",
    "    }\n",
    "\n",
    "每更新一次梯度，仅为了得到梯度都需要计算 m 次偏导数，导致当 m 非常大时，此算法带来非常大的计算量，导致需要很长时间才能得到结果。\n",
    "\n",
    "#### 随机梯度更新(stochastic gradient descent)\n",
    "\n",
    "> 为什么这里随机用stochastic，不用random，请看知乎[「Stochastic」与「Random」有何区别？](https://www.zhihu.com/question/20675303)\n",
    "\n",
    "随机梯度更新时，循环 m 次，每循环一次，计算一次梯度，只需要计算当前样本的偏导数即可：$\\large \\nabla_j = h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}_j $\n",
    "\n",
    "更新算法如下：\n",
    "            \n",
    "    loop until 误差收敛 {\n",
    "        loop i=1 to m {   \n",
    "$\\qquad\\qquad\\quad \\theta_j := \\theta_j - h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}_j $  \n",
    "\n",
    "        }          \n",
    "    }\n",
    "    \n",
    "每更新一次梯度，只需要计算一次，当循环 m 次时，已经更新了 m 次梯度，相比批量梯度更新，此算法计算量相对较小，可以较早到达收敛区间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 代码演示梯度下降求参数\n",
    "\n",
    "### 生成样例数据\n",
    "有在 $y=2x_1 + \\frac{1}{2}x_2^2$ 上的点集，生成点数据代码如下:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 特征数据\n",
    "x_train = [(1.0,3.0), (2.0, 4.0), (3.0, 5.0), (5.0,9.0), (8.0, 10.0)]\n",
    "# 真实值\n",
    "y_train = []\n",
    "for x in x_train:\n",
    "    y_train.append(2*x[0] + ((x[1] ** 2))/2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示样例数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1      x2   y\n",
      "((1.0, 3.0), 6.5)\n",
      "((2.0, 4.0), 12.0)\n",
      "((3.0, 5.0), 18.5)\n",
      "((5.0, 9.0), 50.5)\n",
      "((8.0, 10.0), 66.0)\n"
     ]
    }
   ],
   "source": [
    "# 显示数据    \n",
    "data = zip(x_train, y_train)\n",
    "print \"x1      x2   y\"\n",
    "for d in data:\n",
    "    print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义假设函数  \n",
    "假定我们已知函数的模型为 $h(x) = \\theta_0 x_1 + \\theta_1 x_2^2$ ，但不知参数 $\\theta_0$ 、$\\theta_1$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(x, ** params):\n",
    "    theta_0 = params['theta_0']\n",
    "    theta_1 = params['theta_1']\n",
    "    return theta_0 * x[0] + theta_1 * (x[1] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义代价函数\n",
    "$J(\\theta)=\\frac{1}{2} \\sum \\limits_{i=0}^{m}(h(x^{i})-y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义梯度\n",
    "对任意一项 $\\theta_j$ 代价函数求偏导数：  \n",
    "$\\large \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{2} \\cdot 2 \\cdot (h(x^{i})-y^{(i)}) \\cdot \\frac{\\partial (\\theta_0 x_1^{(i)} + \\theta_1 (x_2^{(i)})^2 - y^{(i)})}{\\partial \\theta_j} = (h(x^{i})-y^{(i)}) \\frac{\\partial (\\theta_0 x_1^{(i)} + \\theta_1 (x_2^{(i)})^2 - y^{(i)})}{\\partial \\theta_j}$\n",
    "\n",
    "此处由于 $x_1$ 、$x_2$ 的指数不一样，所以分别求偏导：\n",
    "\n",
    "$\\large \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = (h(x^{i})-y^{(i)}) x_1^{(i)}$  \n",
    "\n",
    "$\\large \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = (h(x^{i})-y^{(i)}) (x_2^{(i)})^2$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_gradient_update(x_train, y_train, hf, ** params):\n",
    "    delta_0 = 0.0\n",
    "    delta_1 = 0.0\n",
    "    theta_0 = params['theta_0']\n",
    "    theta_1 = params['theta_1']\n",
    "    alpha = params['learning_rate']\n",
    "    for i, x in enumerate(x_train):\n",
    "        delta = alpha * (y_train[i] - hf(x, ** params))\n",
    "        delta_0 += delta * x[0]\n",
    "        delta_1 += delta * (x[1] ** 2)\n",
    "    \n",
    "    theta_0 -= delta_0 * params['flag']\n",
    "    theta_1 -= delta_1 * params['flag']\n",
    "    #print \"delta={} delta_0={} delta_1={}\".format(delta, delta_0, delta_1)\n",
    "    return {'theta_0':theta_0, 'theta_1': theta_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_update(x_train, y_train, hf, ** params):\n",
    "    def circular_locate(times, length):\n",
    "        '''环形方式计算下标，time超过length时，则回到头部重新开始计数'''\n",
    "        return times % base if(times < length) else times - int(times/length)*5\n",
    "\n",
    "    delta_0 = 0.0\n",
    "    delta_1 = 0.0\n",
    "    theta_0 = params['theta_0']\n",
    "    theta_1 = params['theta_1']\n",
    "    alpha = params['learning_rate']\n",
    "    i = locate(params['times'], len(x_train))\n",
    "    x = x_train[i]\n",
    "    delta = alpha * (y_train[i] - hf(x, ** params))\n",
    "    delta_0 += delta * x[0]\n",
    "    delta_1 += delta * (x[1] ** 2)\n",
    "    \n",
    "    theta_0 -= delta_0 * params['flag']\n",
    "    theta_1 -= delta_1 * params['flag']\n",
    "    #print \"delta={} delta_0={} delta_1={}\".format(delta, delta_0, delta_1)\n",
    "    return {'theta_0':theta_0, 'theta_1': theta_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 误差函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squared_error(x_train, y_train, hf, ** params):\n",
    "    error = 0.0\n",
    "    for i, x in enumerate(x_train):\n",
    "        error += (y_train[i] - hf(x, ** params)) ** 2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降主框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x_train, y_train, guf, hf, ef, stop_condition,** params):\n",
    "    times = 1\n",
    "    \n",
    "    def init_real_params(params, times, flag=1.0):\n",
    "        real_params = params.copy()\n",
    "        real_params['flag'] = flag\n",
    "        real_params['times'] = times\n",
    "        return real_params\n",
    "    \n",
    "    real_params = init_real_params(params, times)\n",
    "    \n",
    "    #保留最近一次error\n",
    "    last_error = None\n",
    "    #保留最近一次 real_params\n",
    "    last_params = None\n",
    "    #智能修正次数\n",
    "    smart_correction_times = 0\n",
    "    while(True):        \n",
    "        error = ef(x_train, y_train, hf, ** real_params)\n",
    "        if(error < stop_condition['accept_error']):\n",
    "            print \"found parameters. error={:.8f} times={} smart_correction_times={}\".format(\n",
    "                error, times, smart_correction_times)\n",
    "            break\n",
    "        if(times > stop_condition['max_times']):\n",
    "            print \"reach max times. times = {}\".format(times)\n",
    "            return None\n",
    "        \n",
    "        params_updated = guf(x_train, y_train, hf, ** real_params)\n",
    "        #print \"times = {} error={:.2f} params_updated = {}\".format(times, error, str(params_updated))\n",
    "        \n",
    "        # 智能修正\n",
    "        # 当error越来越大时，反方向更新梯度，并重置参数为最近一次参数（还可以减缓学习速率？）\n",
    "        if(last_error != None and error > last_error):            \n",
    "            smart_correction_times += 1\n",
    "            #print \"smart correction. last_error={:.8f} error={:.8f}\".format(last_error, error)\n",
    "            #反转符号极性（决定梯度的更新操作是加还减），并使用上一次的参数\n",
    "            real_params = init_real_params(last_params, times, -1)\n",
    "        else:\n",
    "            # 更新参数\n",
    "            real_params.update(params_updated)\n",
    "            real_params['times'] = times\n",
    "            last_params = real_params.copy()\n",
    "            \n",
    "        # 记录本次error\n",
    "        last_error = error\n",
    "        times += 1\n",
    "        \n",
    "    del real_params['times']\n",
    "    return {'times':times, 'smart_correction_times': smart_correction_times, 'params': real_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程序入口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= batch_gradient_update ==============\n",
      "found parameters. error=0.00009994 times=17202 smart_correction_times=1\n",
      "{'smart_correction_times': 1, 'params': {'flag': -1, 'theta_0': 1.9945897310077012, 'theta_1': 0.500407941007894, 'learning_rate': 0.0001}, 'times': 17202}\n",
      "\n",
      "=========== stochastic_gradient_update ===========\n",
      "found parameters. error=0.00010000 times=276426 smart_correction_times=110569\n",
      "{'smart_correction_times': 110569, 'params': {'flag': -1, 'theta_0': 1.9948595782513392, 'theta_1': 0.5004111433971709, 'learning_rate': 0.0001}, 'times': 276426}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "theta_0 = 0.1\n",
    "theta_1 = 0.1\n",
    "gradient_descent_algorithms = [batch_gradient_update, stochastic_gradient_update]\n",
    "for alg in gradient_descent_algorithms:\n",
    "    print \"{:=^50s}\".format(\" \" + alg.__name__ + \" \")\n",
    "    \n",
    "    out_params = gradient_descent(x_train, y_train, alg, h, squared_error, \n",
    "    {'max_times':1000000, 'accept_error': 0.0001}, \n",
    "    theta_0=theta_0, theta_1=theta_1, learning_rate = 0.0001)\n",
    "    \n",
    "    \n",
    "    print out_params\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
